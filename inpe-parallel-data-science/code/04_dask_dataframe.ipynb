{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask DataFrames\n",
    "\n",
    "Similar to the design of arrays, dask dataframes are paritioned pandas dataframes. Computations run pandas operators on the chunks and aggregate results from the chunked operations.\n",
    "\n",
    "<img src=\"https://dask.org/_images/dask-dataframe.svg\" width=\"256\" title=\"    https://dask.org/_images/dask-dataframe.svg\" />\n",
    "\n",
    "For the most part, dask has tried to implement all of pandas, but there are some inefficient operations that it does not support.\n",
    "\n",
    "Let's load up a dataframe and see what we get.  This is the NYC flight data used in the dask tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_csv('../input/dask-nyc-flights/*.csv',\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={'TailNum': str,\n",
    "                        'CRSElapsedTime': float,\n",
    "                        'Cancelled': bool})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe has 10 chunks (npartitions) that correspond to the ten files that were read.  Dataframes have two key properties:\n",
    "  * they are tabular (two-dimensional) data\n",
    "  * each column has a datatype defined by a _schema_\n",
    "  \n",
    "The programmer is encouraged to think of this as \"spreadsheet or SQL table\".  It is reasonable to call the data __structured__.\n",
    "  * this is in contrast to semi-structured data which has tags and hierarchy, but does not enforce types. Examples are XML and JSON.\n",
    "  * in database parlance, this is flat data model.\n",
    "\n",
    "### Data Slicing and Aggregation\n",
    "\n",
    "The most basic operations in a database and in dask is to <code>select</code> rows and <code>project</code> columns.\n",
    "\n",
    "#### Selecting rows\n",
    "\n",
    "Find all flights flown by a specific plane, identified by <code>TailNum</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.TailNum=='N516UA'].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projecting Columns\n",
    "\n",
    "Build a dataframe that describes the plane ('TailNum') and route ('FlightNum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes = df[['FlightNum','TailNum']].compute()\n",
    "routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating Data\n",
    "\n",
    "A common data science inquiry is to query an aggregate (mean, min, max, sum, etc.) in a group.  This is done with a <code>groupby</code> query. The pattern is to construct a grouping and then aggregate over the grouping.\n",
    "\n",
    "__Query:__ How many different flights were flown by each plane?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes.groupby('TailNum').FlightNum.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have intentionaly mixed syntax. Dataframes in both R and Python use two form of syntax interchangeably.\n",
    "  * <code>dataframe.columnName</code>\n",
    "  * <code>dataframe['columnName']\n",
    "\n",
    "Many functions only accept the bracketed indexing of columns.\n",
    "    \n",
    "__Query:__ How many different planes flew each flight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes.groupby('FlightNum')['TailNum'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Query:__ What were the most routes flown by a single plane?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes.groupby('TailNum')['FlightNum'].count().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Query:__ What is the maximum number of planes to fly a single route?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes.groupby('FlightNum')['TailNum'].count().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, these aggregates are not really the questions we want answered. More natural questions are awkward.\n",
    "  * What plane flew the most routes?\n",
    "  * What route was flown by the most planes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes.groupby('FlightNum').TailNum.count().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes.groupby('TailNum').FlightNum.count().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reveals problems with the data.  So, let's look for an actual plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes[routes.TailNum != 'UNKNOW'].groupby('TailNum').FlightNum.count().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and ask for how many flights it has flown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes[routes.TailNum != 'UNKNOW'].groupby('TailNum').FlightNum.count().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and verify that this is the right answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routecount = routes.groupby('TailNum').FlightNum.count()\n",
    "routecount.get('N413DA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have uncovered what I think is the most annoying part of dask and pandas dataframes. Aggregate functions return series, which are not dataframes. They have different methods.  I would have preferred to have written:\n",
    "\n",
    "<code>routecount[routecount.TailNum=='N413DA'].compute()</code>\n",
    "\n",
    "But, that's dataframe syntax not series syntax.  Aggregates assume that the output is small and return pandas series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(routecount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexes\n",
    "\n",
    "All dataframes have a __default index__.  In this case, the index was generated when we loaded the data and is the row number in the pandas dataframe. Surprisingly, the index is not unique.  The same index value appears in each pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.index, \"\\n\")\n",
    "print(\"Number of rows in the database\\n\", len(df))\n",
    "maxindex = df.index.nunique().compute()\n",
    "print(\"Number of unique values in the index\\n\", maxindex)\n",
    "\n",
    "# find all entries with index value 22000\n",
    "df.loc[22000].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of an index comes from relational databases: \"A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure.\"\n",
    "\n",
    "Real indexes come in many forms, but are most commonly:\n",
    "  * hash tables -- organize the data by the hash value of a key field for constant time $O(1)$ lookup by key.\n",
    "  * B+-trees/sorted -- sort the data in a tree to lookup a key in $O(\\log n)$ time and be able to scan sequential keys.\n",
    "\n",
    "A blog from TimesTen gives a reasonable schematic.\n",
    "\n",
    "<img src=\"https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/bbeb190a-b93b-4d7b-bd6c-3f9928cd87d2/Image/fdf8758152659ecde76a20de8c60c23b/which_is_best.JPG\" width=\"512\" title=\"https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/bbeb190a-b93b-4d7b-bd6c-3f9928cd87d2/Image/fdf8758152659ecde76a20de8c60c23b/which_is_best.JPG\" />\n",
    "\n",
    "Indexing can be accomplished in dask by calling <code>set_index</code>, which often results in a global shuffle of the data.  In can be very expensive.  The data that we have is already sorted by date, so setting the index to that value does not take a long time. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask has implemented some what I would expect out of indexing:\n",
    "1. dask can be told that there is existing structure in the data\n",
    "2. dask reorganizes data to optimize queries\n",
    "\n",
    "Dask does not support some features that I think it should\n",
    "1. hash indexing\n",
    "2. data readers infer the existence of an index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do indexes matter?\n",
    "\n",
    "Understanding the structure of the can lead to massive performance differences.\n",
    "We can create an example in which seemingly identical queries perform \n",
    "differently because of an implicit index structure.\n",
    "\n",
    "We create 100 files each with 100000 entries with two fields 'A' and 'B'.  'A' contains an integer that identifies the file number (0-99). 'A' is the same in each file. 'B' contains a sequence of numbers 0-1000000 in each files. These data:\n",
    "  * are too big to fit in memory\n",
    "  * have one field that will be identical in each pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv('../output/untracked/*.csv')\n",
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare how long it takes to sum all the elements grouped by each key for both columns.  In both cases, we touch all of the data.  But, the sums in when we <code>groupby</code> 'A' can all be evaluated in one chunk.  When we <code>groupby</code> 'B', partial sums at each value must be aggregated across all chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.groupby('A').sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.groupby('B').sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('A')\n",
    "%time df.groupby('A').sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are big performance differences that come from both:\n",
    "  * the organization of the data\n",
    "  * dask knowing about the organization of the data\n",
    "\n",
    "### Parting Thoughts\n",
    "\n",
    "Limitations and comments:\n",
    "  * dask does not have a general sort capability\n",
    "    * but, can be accomplished with set_index\n",
    "    * shuffle is inefficient, use a different engine\n",
    "  * dask does not support row indexing by phycial offset\n",
    "    * this is not an important feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data\n",
    "\n",
    "Uncomment and run this once to make data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv\n",
    "#for i in range(100):\n",
    "#    with open(f'../output/untracked/csv{i}.csv', 'w', newline='') as csvfile:\n",
    "#        csvw = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "#        csvw.writerow(['A','B'])\n",
    "#        for j in range(1000000):\n",
    "#            csvw.writerow([i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
